\documentclass[12 pt]{report}
\usepackage[margin=1in, paperwidth=8.3in, paperheight=11.7in]{geometry}
\usepackage[colorlinks]{hyperref}

\usepackage{pgf}
\usepackage{pgfpages}
\usepackage{ragged2e}
\usepackage{hyperref}
\bibliographystyle{plain}
\pgfpagesdeclarelayout{boxed}
{
  \edef\pgfpageoptionborder{0pt}
}
{
  \pgfpagesphysicalpageoptions
  {
    logical pages=1,
  }
  \pgfpageslogicalpageoptions{1}
  {
    border code=\pgfsetlinewidth{3pt}\pgfstroke,
    border shrink=\pgfpageoptionborder,
    resized width=0.9\pgfphysicalwidth,
    resized height=0.9\pgfphysicalheight,
    center=\pgfpoint{0.5\pgfphysicalwidth}{.5\pgfphysicalheight}
  }
}
\pgfpagesuselayout{boxed}

\usepackage{graphicx}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true, 
    linktoc=all,     
    linkcolor=black, 
    citecolor=black,
}

\begin{document}

\begin{center}
  \includegraphics[width=6.3in]{BVB.png}
\end{center}


\begin{center}
  \begin{Large}
    \textbf{\linebreak School of
      \linebreak Electronics and Communication Engineering }
  \end{Large}
\end{center}

\begin{center}
  \begin{Large}
    \textbf{\linebreak \linebreak  Mini Project Report \vspace{0.5cm}\\on}
  \end{Large}
\end{center}

\begin{center}
  \begin{LARGE}
    \textbf{{Traffic Light Detection in Diverse Weather Conditions Using Machine Learning  \linebreak\\}}
  \end{LARGE}
\end{center}
\vspace{2cm}
\begin{flushleft}
  \textbf{By:}
  \begin{large}
    \begin{enumerate}
      \item \textbf{Naveenkumar Gumaste}      \hspace{2.2cm}USN:01FE22BEC407
      \item \textbf{Chandan C Raikar}         \hspace{3.38cm}USN:01FE21BEC038
      \item \textbf{Divya R Salmani}          \hspace{3.81cm}USN:01FE21BEC013
      \item \textbf{Sanjana Adagimath }       \hspace{3.0cm}USN:01FE21BEC363
    \end{enumerate}
    \vspace{1cm}
  \end{large}
  \begin{Large}
    \textbf{Semester: V, 2023-2024}
  \end{Large}
\end{flushleft}

\begin{flushright}
  \begin{Large}
    Under the Guidance of \linebreak \\
    \textbf{Prof.Rajeshwari K}
  \end{Large}
\end{flushright}

\newpage
\begin{center}
  {\color{brown} \textbf{K.L.E SOCIETY'S\\
      \begin{small}KLE Technological University,\\
      \end{small}
      HUBBALLI-580031\\}}
  \begin{large}
    {\color {brown} \textbf{2022-2023 \linebreak}}

    \includegraphics[width=1in]{certkle.png}
    \ \\
    \ \ \ \\
    SCHOOL OF ELECTRONICS AND COMMUNICATION ENGINEERING \linebreak
  \end{large}

  \begin{Large}
    {\color {violet} \textbf{CERTIFICATE}} \linebreak
  \end{Large}
  \begin{center}
    \justify{This is to certify that project entitled \textbf{“Traffic Light Detection in Diverse Weather Conditions Using Machine Learning”} is a bonafide work carried out by the student team of "\textbf{Naveenkumar Gumaste(01FE22BEC407)}, \textbf{Chandan C Raikar\linebreak(01FE21BEC038)}, \textbf{Divya R Salmani (01FE21BEC013)}, \textbf{Sanjana Adagimath (01FE21BEC363)}" . The project report has been approved as it satisfies the requirements with respect to the mini project work prescribed by the university curriculum for BE (V Semester) in School of Electronics and Communication Engineering of KLE  \linebreak Technological University for the academic year 2023-2024.}
  \end{center}

\end{center}

\vspace{2cm}
\begin{small} \textbf{Prof. Rajeshwari K} \hspace{1.5cm} \textbf{Dr. Suneeta V Budihal} \hspace{1.5cm} \textbf{Dr. B. S. Anami} \linebreak
\end{small}
\vspace{1cm}
%\begin{large}
\hspace{1.8cm} \small{\textbf{Guide} \hspace{3.8cm} \textbf{Head of School} \hspace{3.2cm}  \textbf{Registrar}}
%\end{large}


\vspace{0.8cm}

\begin{flushleft}
  \textbf{External Viva: \\}
\end{flushleft}
\textbf{Name of Examiners} \hspace{8cm} \textbf{Signature with date}
\begin{enumerate}
  \item
  \item
\end{enumerate}

\newpage
\begin{center}
  \begin{Large}
    \textbf{ACKNOWLEDGEMENT}
  \end{Large}
\end{center}
We extend our gratitude to Dr. Ashok Shettar, Vice-Chancellor of KLE Technological University, Hubballi, and Dr. P. G. Tewari, Dean Academics of KLE Technological University, for providing us with the opportunity to conduct our research and for their unwavering support throughout the CIM Mini Project. Special thanks to Dr. Suneeta V.B., our head of school, for her leadership and support. We are also thankful to Prof. Rajeshwari K. for her invaluable help and guidance during our work. Additionally, we acknowledge the contributions of all teaching and non-teaching staff for their assistance and encouragement.

\flushright
-The project team
\flushleft


\newpage
\begin{center}
  \begin{Large}
    \textbf{ABSTRACT}
  \end{Large}
\end{center}
\justify
This project presents a current exploration into the critical realm of traffic light detection within intelligent transportation systems (ITS), which is crucial for enhancing road safety and traffic management. Motivated by the continuous evolution of methodologies, our research provides a comprehensive analysis of detection techniques. From traditional computer vision to deep learning integration, we scrutinize strengths, limitations, and potential future directions. The study is conducted using datasets from ROBOFLOW UNIVERSE, demonstrating novelty in the approach. Ongoing advancements in detection strategies are crucial for improving road safety and traffic efficiency, establishing this research's significance within the evolving landscape of intelligent transportation systems.
\flushright

\newpage
\tableofcontents

\flushleft
\newpage
\chapter{Introduction}
\justify
In the Intelligent Transportation Systems (ITS) domain, the efficiency of traffic light detection is integral to advancing road safety and optimizing traffic flow. The burgeoning growth of urban environments amplifies the demand for sophisticated systems capable of managing intricate traffic networks. This project responds to this imperative by concentrating on elevating the precision and efficiency of traffic light detection, specifically addressing challenges posed by urban mobility. The methodology employed involves a thorough examination of detection approaches, spanning from traditional computer vision to cutting-edge deep learning. This exhaustive analysis explores the strengths, limitations, and applications of Intelligent Transportation Systems (ITS). By leveraging datasets from ROBOFLOW UNIVERSE, the project ensures its relevance in real-world scenarios. Integrating traditional computer vision with deep learning not only contributes valuable insights but also positions the project to make meaningful contributions within the dynamic field of ITS.
\flushleft
\section{Motivation}
\justify
Improving the precision and efficiency of traffic light detection within Intelligent Transportation Systems (ITS) is pivotal for enhancing road safety in increasingly complex urban environments. As urban areas expand, the demand for advanced systems to manage intricate traffic networks grows, compelling this project to focus on optimizing traffic flow through enhanced traffic light detection. The motivation stems from addressing urban mobility challenges, prompting a thorough review of detection approaches, and leveraging datasets from Roboflow Universe to develop solutions relevant to real-world scenarios. Integrating traditional computer vision with cutting-edge deep learning technologies adds a novel dimension, offering valuable insights and advancing traffic light detection methodologies within the dynamic realm of Intelligent Transportation Systems (ITS). In essence, this project is driven by the overarching goals of fostering road safety, optimizing traffic flow, addressing urban mobility challenges, and integrating innovative technologies to contribute meaningfully to the evolution of ITS.
\flushleft

\section{Objectives}
\begin{justify}
\begin{itemize}
  \item \textbf{Precision in Detection:} Formulating algorithms that exhibit high precision in detecting the presence of traffic lights within captured images, aiming to enhance road safety and traffic flow optimization significantly.

  \item \textbf{Advanced Image Processing:} Developing sophisticated algorithms for extracting relevant features from input images. The system must demonstrate robustness in handling variations such as occlusions, distortions, and changes in perspective, ensuring accurate and reliable traffic light detection.

  \item \textbf{Intelligent Classification:} Implementation of classification algorithms to intelligently recognize the specific type and significance of each detected traffic light. This step is crucial for providing nuanced insights into the traffic signal status and facilitating effective decision-making in Intelligent Transportation Systems (ITS).

  \item \textbf{Robust Adaptability:} Ensuring the system's robustness and adaptability to diverse environmental conditions, encompassing different lighting scenarios, weather variations, and changes in the appearance of traffic lights. The objective is to create a system that performs reliably in real-world settings.
\end{itemize}.
\end{justify}

\section{Literature survey}
\justify
 {
  \hspace*{1cm}{\bfseries Mark P Philipsen}, proposed the learning-based detector outperformed heuristic model-based detectors, exhibiting superior precision and recall—a critical parameter due to the irrevocable loss of false negatives. Emphasizing the significance of the learning-based detector’s heightened recall, the study primarily assessed its success in detecting traffic lights. Proposed enhancements involved incorporating tracking methods to refine its output. The overall system performance was evaluated through precision-recall curves and the Area Under the Curve (AUC), providing a comprehensive analysis of the effectiveness of the implemented learning-based detector and associated methodologies.

  \hspace*{1cm}{\bfseries Trung-Hieu Nguyen’s}, proposed real-time traffic sign recognition model integrated into a 1:7 RC vehicle, demonstrated remarkable effectiveness and robustness in challenging scenarios. The system exhibited an impressive average accuracy of 99.78 percent in detecting traffic signs on the embedded platform. Despite its success, limitations were identified, including the impracticality of many existing systems in real-time environments, the computational weight of deep learning methods on embedded systems, consideration of only five common traffic signs, and a response time of 22 to 23 frames per second. The study advocates for further system expansion and methodology refinement, emphasizing a lightweight model, optimal recognition methods, and image processing techniques for training data.

  \hspace*{1cm}{\bfseries Noor Hussain Sarhan’s} study, the primary focus was on evaluating and comparing two traffic light detection models, emphasizing their accuracy in classifying images and video frames. The findings centered on the precision of traffic light detection in both static images and video frames. Identified limitations included the necessity for further research to amalgamate the strengths of the two models and the call for future investigations employing deep learning algorithms and an expanded dataset. The methodology consisted of developing two distinct models utilizing the OpenCV library for image and video processing, with the YOLO detection system fine-tuning the video model’s weights and employing predefined color ranges for traffic light detection in images.

  \hspace*{1cm}{\bfseries  Amara Dinesh’s}, research highlights the success of capsule networks, achieving a state-of-the-art accuracy of 97.6 percent on the German traffic sign dataset. Capsule networks are superior to CNNs in the challenging task of traffic sign detection, excelling in recognizing pose and spatial variances. This enhances reliability and accuracy in image classification, even for blurred, rotated, and distorted images. The study evaluates the algorithm’s performance in different orientations, emphasizing its proficiency in correctly identifying traffic signs. Limitations include inherent CNN limitations, the need for manual feature engineering, an unbalanced test set, and relatively lengthy training times. Methodologically, capsuleFig. 1. Functional Block Diagram networks are employed with specific layers, a route-by-agreement algorithm, and a decoder network.
 }

\section{Problem statement}
Traffic Light Detection in Diverse Weather Conditions Using Machine Learning.
\section{Organization of the report}
\justify

\begin{itemize}
  \item \textbf{Chapter 1: Introduction}
        - Initiating with an introduction, the project provides an initial overview, setting the stage for the systematic exploration of various facets.

  \item \textbf{Chapter 2: System Design}
        - The project extensively examines the complexities of system design, offering an in-depth overview of its structure and components.

  \item \textbf{Chapter 3: Planning and Execution}
        - In this thoroughly covers the planning and execution aspects of the design, shedding light on strategic decisions made during the development phase.

  \item \textbf{Chapter 4: Results Analysis}
        - Progressing further shifts focus to presenting and analyzing results, featuring detailed discussions to clarify findings and highlight significant outcomes.

  \item \textbf{Chapter 5: Conclusion}
        - Finally concluding the report with a summary, providing insights into project outcomes, implications, and paving the way for future exploration and development in the identified scope.
\end{itemize}




\newpage
\chapter{System design}
\justify
In this Chapter, We will analyze YOLOV8's cutting-edge advancements and intricate design principles as we look into the system's inner workings. The block diagram for our project will show how YOLOV8's cutting-edge features were incorporated and how well our solution integrated them.

\section{Functional Block Diagram}
\begin{figure}[h]
  \centering
  \includegraphics[width=1.08\textwidth]{Fun-Diagram.png}
  \caption{Shows Functional Block Diagram}
  \label{fig:function_block_diagram}
\end{figure}
\subsection{Functional Block Diagram Explanation}
{
  \begin{justify}
  \begin{itemize}
    \item \textbf{Input:} Video frames or still images, maybe in different resolutions and formats (such as RGB or GRAYSCALE), are received.

    \item \textbf{Preprocessing:} Applies data normalization, and filtering (e.g., Gaussian blur) to mitigate noise and enhance relevant features. May involve color space conversion (e.g., RGB to HSV) for robust traffic light representation.

    \item \textbf{Region of Interest (ROI) Detection:} Employs object detection algorithms (e.g., YOLOV8) to localize traffic light bounding boxes within the frame. Can incorporate anchor boxes of varying sizes and aspect ratios to handle diverse traffic light positions and appearances.

    \item \textbf{Traffic Light Classification:} Leverages deep convolutional neural networks (CNNs) trained on labeled datasets of traffic light images. CNN architecture likely features convolutional layers for feature extraction, followed by pooling layers for spatial down-sampling and fully-connected layers for classification into red, yellow, or green states.
  \end{itemize}
  \end{justify}
}

\section{YoloV8 Architectural Diagram}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{Yolov8_Arch.png}
  \caption{Shows YoloV8 Architectural Diagram}
  \label{fig:function_block_diagram}
\end{figure}
\subsection{YoloV8 Architectural Diagram Explanation}
{
  \begin{justify}
    \item \textbf{Backbone:}
    \begin{itemize}
        \item The backbone of YOLOv8 plays a pivotal role in feature extraction from input images.
        \item Comprising multiple convolutional layers, it systematically increases channel numbers, enhancing the model's ability to capture intricate patterns.
        \item The backbone's architecture, whether adopting ResNet or CSPDarknet, may vary based on the specific model version.
    \end{itemize}

\item \textbf{Neck:}
    \begin{itemize}
        \item Following the backbone, the neck processes the extracted features, preparing them for the subsequent detection head.
        \item Incorporating a spatial pyramid pooling (SPP) layer, the neck aggregates features from diverse spatial scales, contributing to a comprehensive understanding of the image.
        \item Additional convolutional layers further refine and shape the features for effective object detection.
    \end{itemize}

\item \textbf{Head:}
    \begin{itemize}
        \item The head of the YOLOV8 model is responsible for making the final predictions based on the processed features.
        \item Comprising a series of convolutional layers, it predicts crucial information such as bounding boxes, class probabilities, and confidence scores for identified objects.
    \end{itemize}

\item \textbf{C2f module:}
    \begin{itemize}
        \item A recent advancement in YOLOV8, the C2f module enhances the model's accuracy by merging characteristics from both the neck and backbone.
        \item This integration provides a more nuanced and illuminating depiction of the image, contributing to improved object recognition and localization.
    \end{itemize}

\item \textbf{Decoupled head:}
    \begin{itemize}
        \item Another noteworthy addition to the YOLOV8 architecture is the decoupled head, designed to optimize model speed without compromising performance.
        \item By eliminating the need for a separate objectness branch, which can be computationally expensive, the decoupled head contributes to overall model efficiency.
    \end{itemize}
  \end{justify}
}

\newpage
\chapter{Implementation details}
\justify
In this chapter, We will dive into the technical intricacies of establishing a robust environment tailored for training a YOLOV8 model. This involves configuring essential dependencies, preparing the dataset meticulously, and executing the training and testing process for the model.
\section{Environment Setup}
 {
  \begin{enumerate}
    \item \textbf{ Operating system library for interacting with the OS:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{import Os.png}
          \end{center}

    \item \textbf{using it for file path expansion:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{Import global.png}
          \end{center}

    \item \textbf{Displaying images in IPython environment:}
          \begin{center}
            \includegraphics[width=17cm, height=0.8cm]{Form Iphoton.display.png}
          \end{center}

    \item \textbf{ It displays detailed information about each GPU on the system, such as its name, model, temperature, utilization, memory usage, and more:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{nvidia-smi.png}
          \end{center}

    \item \textbf{Installing and updating the Ultralytics library:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{ipi install ultalatics.png}
          \end{center} 
\newpage
    \item \textbf{Importing Ultralytics into the project:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{import ultralatics.png}
          \end{center}

    \item \textbf{Checking if the Ultralytics is correctly setup:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{check ultralatics.png}
          \end{center}

    \item \textbf{Setting Constants, Creating Directories and Changing Directory :}
          \begin{center}
            \includegraphics[width=17cm, height=2cm]{creating home and dataset folder.png}
          \end{center}
  \end{enumerate}
 }

\section{Setting Up YoloV8, Dataset, Training YoloV8s Model And Getting The Results}
 {
  \begin{enumerate}
    \item \textbf{Installing the Roboflow library with the dataset using its unique api-key:}
          \begin{center}
            \includegraphics[width=17cm, height=2.5cm]{Pip insall roboflow.png}
          \end{center}
          \subsection{Setting up YoloV8, Training YoloV8s model}
    \item \textbf{Training the YOLOV8 model with specified parameters of 125 epochs with an image size of 1000 pixels and a batch size of 32:}
          \begin{center}
            \includegraphics[width=17cm, height=1.5cm]{Training Yolov8model.png}
          \end{center}
    \item \textbf{Listing Detected Objects/files:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{Listing out the files created after trining .png}
          \end{center}

          \vspace{1.34cm}
          \subsection{Visualizing Results In The Environment}
    \item \textbf{Displaying confusion matrix using "Image" command imported from IPython.display:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{displaying the confusion matrix in environment.png}
          \end{center}

    \item \textbf{Validating the model in validation mode using a trained model:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{validating the yolomodel on the dataset.png}
          \end{center}

    \item \textbf{Predicting on an "Image" in prediction mode on a single image file:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{Testing the model on real-world img.png}
          \end{center}

    \item \textbf{Predicting on a "Video" in prediction mode on single video file:}
          \begin{center}
            \includegraphics[width=17cm, height=1cm]{Testing the model on real-world video.png}
          \end{center}

  \end{enumerate}
 }


\newpage
\chapter{Results}
\justify
 {
  In this chapter, we will delve into a comprehensive exploration of the results and project outcomes achieved through the implementation of YOLOV8. This entails an in-depth analysis of the results obtained.
 }
\section{Visualizing Results}
 {
  \begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{Results-Collgess.jpg}
    \caption{Shows results of YoloV8 implementation for our real-world sample images}
    \label{fig:Results achieved from YoloV8s implementation}
  \end{figure}
  \justify
  \hspace*{1cm} fig 4.1 shows a few sample results of our YoloV8 implementation on our own images other than the dataset images. The results are quite good, and they can be used in real-world applications.
  \newpage
  \subsection{Result Analysis}
  {
    \begin{figure}[h]
      \centering
      \includegraphics[width=0.8\textwidth]{confusion_matrix_normalized.png}
      \caption{Shows confusion matrix of YoloV8s trained model}
      \label{fig:Confusion Matrix YoloV8 Trained Model}
    \end{figure}
    \justify
    \hspace*{1cm} fig 4.3 shows the confusion matrix of our YoloV8 trained model which is trained on our own custom dataset. The confusion matrix shows the accuracy of our model, which averages at 96\%.

    \begin{figure}[h]
      \centering
      \includegraphics[width=0.5\textwidth]{Comaprison Results.png}
      \caption{Shows results comparison of epochs for which the model is trained}
      \label{fig: YoloV8 implementation}
    \end{figure}

    \begin{justify}
    \hspace*{1cm} fig 4.3 shows that study showcased significant improvements in Mean Average Precision (MAP) scores across training epochs. Starting at a MAP score of 0.345 after 50 epochs, the detection accuracy steadily increased, reaching 0.962 at 125 epochs. This trajectory underscores the efficacy of our methodology in advancing traffic light detection accuracy, emphasizing the ongoing importance of evolving detection strategies for optimizing safety and traffic efficiency in the dynamic landscape of ITS(Intelligent Transportation Systems).
    \end{justify}
  }
 }



\newpage
\chapter{Conclusion and future scope}
\section{Conclusion}
\justify
 {
  \hspace*{1cm} In conclusion, our thorough investigation and the corresponding results highlight the significant potential of combining conventional and deep learning techniques for robust traffic light detection. This research contributes valuable insights to the broader field of Intelligent Transportation Systems (ITS), aiming to enhance the accuracy of detection systems and ultimately contribute to the creation of safer and more efficient transportation systems. By examining both well-established and advanced learning methodologies, our study provides a nuanced understanding of their collaborative roles. This understanding is pivotal in optimizing the precision and reliability of traffic light detection. As we navigate the evolving landscape of transportation technology, the outcomes of this research illuminate pathways for further advancements, ensuring the continual improvement of safety measures and efficiency within the realm of Intelligent Transportation Systems.
 }

\begin{flushleft}
\end{flushleft}
\newpage
\section{Future Scope}
\begin{justify}
\begin{enumerate}
    \item \textbf{Enhanced Dataset Diversity:}
    Recommending the utilization of a more diverse dataset with increased variability in environmental conditions, lighting, and traffic scenarios to improve the robustness and generalization of the traffic light detection model.
    \item \textbf{Detailed Annotation and Bounding Boxes:}
    Emphasizes the importance of meticulous annotation and precise bounding boxes in the dataset to provide a richer ground truth for training, ensuring the model's accuracy in identifying and localizing traffic lights under various circumstances.
    \item \textbf{High-Quality Hardware Implementation:}
    Suggesting the adoption of advanced hardware configurations, possibly leveraging GPUs or TPUs, to expedite model training and inference processes. This can lead to enhanced efficiency and real-time performance, especially in scenarios with increased computational demands.
    \item \textbf{Integration of Advanced Learning Techniques:}
    Exploring the incorporation of state-of-the-art deep learning techniques, such as transfer learning or ensemble methods, to further enhance the model's ability to generalize across diverse traffic light scenarios and potentially improve its performance on challenging instances.
    \item \textbf{Continuous Model Refinement:}
    Encouraging an iterative approach to model refinement through continuous experimentation with hyperparameter tuning, architecture modifications, and advanced training strategies. This approach ensures the adaptability of the traffic light detection model to evolving real-world conditions and challenges.
\end{enumerate}
\end{justify}
\addcontentsline{toc}{chapter}{References}
\newpage
\section*{References}

\begin{justify}
  \textbf{Mark Philip Philipsen,} "Traffic Light Detection: A Learning Algorithm and Evaluations on Challenging Dataset", 2015 IEEE 18th International Conference on Intelligent Transportation Systems.
  \href{https://doi.org/10.1109/ITSC.2015.378}{[1]}\newline
  \textbf{Trung-Hieu Nguyen,} "A Lightweight Model For Real-time Traffic Sign Recognition", 2020 5th International Conference on Green Technology and Sustainable Development (GTSD).
  \href{https://doi.org/10.1109/GTSD50082.2020.9303072}{[2]}\newline
  \textbf{Noor Hussain Sarhan,} "Traffic light Detection using OpenCV and YOLO", 2022 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT).
  \href{https://doi.org/10.1109/3ICT56508.2022.9990723}{[3]}\newline
  \textbf{Amara Dinesh Kumar,} "Novel Deep Learning Model for Traffic Sign Detection Using Capsule Networks," CoRR, vol. abs/1805.04424, 2018.
  \href{https://doi.org/10.48550/arXiv.1805.04424}{[4]}\newline
  \textbf{Fatma Nur Ortataş,} "Performance Evaluation of YOLOv5, YOLOv7, and YOLOv8 Models in Traffic Sign Detection", 2023 8th International Conference on Computer Science and Engineering (UBMK).
  \href{https://doi.org/10.1109/UBMK59864.2023.10286611}{[5]}\newline
  \textbf{Abdellah Nabou,} "Road Signs Recognition by Using YOLOv8 Model", 2023 14th International Conference on Intelligent Systems: Theories and Applications (SITA).
  \href{https://ieeexplore.ieee.org/document/10373723}{[6]}\newline
  \textbf{Ultralytics,} "YOLOv8 Docs", https://docs.ultralytics.com/models/yolov8/.
  \href{https://www.ros.org/}{[7]}
\end{justify}

\end{document}

\addcontentsline{toc}{chapter}{Appendix}
\end{document}
